{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:12:50.026015600Z",
     "start_time": "2023-05-23T14:12:43.912111300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\noudy\\\\PycharmProjects\\\\Cassidy\\\\Cassidy\\\\application')\n",
    "\n",
    "from A_DataCollectors.ScientificLiteratureCollector.scientific_literature_collector import ScientificLiteratureCollector\n",
    "from C_DataProcessors.text_preprocessor import TextPreprocessor\n",
    "from D_Analyzers.Summarization.extractive_summarizer import ExtractiveSummarizer\n",
    "from D_Analyzers.Relation_Extraction.relation_extractor import RelationExtractor\n",
    "\n",
    "\n",
    "class ScientificLiteratureSummarizer:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.format = 'pdf'\n",
    "        self.source_type = 'url' if path.startswith('http') else 'local'\n",
    "        self.method = 'scipy'\n",
    "\n",
    "    def summarize(self):\n",
    "        # Collect data\n",
    "        collector = ScientificLiteratureCollector(self.path)\n",
    "        text = collector.collect(self.format, self.source_type, self.method)\n",
    "\n",
    "        # Preprocess data\n",
    "        summarization_steps = ['clean_data', 'split_sentences']\n",
    "        preprocessor = TextPreprocessor(summarization_steps)\n",
    "        preprocessed_text = preprocessor.preprocess_grobid(text)\n",
    "\n",
    "        # Summarize data\n",
    "        new_dict = {}\n",
    "        for header, sentences in preprocessed_text.items():\n",
    "            es = ExtractiveSummarizer(sentences)\n",
    "            summary = es.summarize('textrank', top_n=3, order_by_rank=False)\n",
    "\n",
    "            # filter out sentences less than four words long\n",
    "            summary = '. '.join(sentence for sentence in summary.split('. ') if len(sentence.split()) >= 3)\n",
    "\n",
    "            new_dict[header] = summary\n",
    "\n",
    "        return new_dict\n",
    "\n",
    "    def relation_extractor(self):\n",
    "        # Collect data\n",
    "        collector = ScientificLiteratureCollector(self.path)\n",
    "        text = collector.collect(self.format, self.source_type, self.method)\n",
    "\n",
    "        # Preprocess data\n",
    "        relation_steps = ['clean_data']\n",
    "        preprocessor = TextPreprocessor(relation_steps)\n",
    "        preprocessed_text = preprocessor.preprocess_grobid(text)\n",
    "        preprocessed_text = preprocessor.concatenate_sections_grobid(preprocessed_text)\n",
    "\n",
    "        relation_extraction_steps = ['clean_data', 'case_folding', 'split_sentences', 'tokenize_sentences', 'pos_tagging', 'filter_pos_tagged']\n",
    "        relation_preprocessor = TextPreprocessor(relation_extraction_steps)\n",
    "        new_text = relation_preprocessor.preprocess_string(preprocessed_text)\n",
    "\n",
    "        # Analyze the data\n",
    "        relation_extractor = RelationExtractor(new_text)\n",
    "        relations = relation_extractor.extract('co_occurrence')\n",
    "\n",
    "        return relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noudy\\PycharmProjects\\Cassidy\\venv\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thousand', 'thousand'), ('decision', 'language'), ('cognates', 'effect'), ('effect', 'similarity'), ('cognates', 'similarity'), ('kroll', 'thousand'), ('frequency', 'similarity'), ('cognate', 'effect'), ('cognates', 'decision'), ('frequency', 'word')]\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/noudy/Downloads/1-s2.0-S0749596X09001247-main.pdf\"\n",
    "\n",
    "relation = ScientificLiteratureSummarizer(path)\n",
    "result = relation.relation_extractor()\n",
    "\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:12:58.233392100Z",
     "start_time": "2023-05-23T14:12:52.248315Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noudy\\PycharmProjects\\Cassidy\\venv\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section: title\n",
      "==============\n",
      "Summary: Evaluation and analysis of term scoring methods for term extraction.\n",
      "\n",
      "\n",
      "Section: Introduction\n",
      "=====================\n",
      "Summary: \n",
      "\n",
      "\n",
      "Section: Our approach\n",
      "=====================\n",
      "Summary: We start by explaining our approach before discussing the term scoring literature and methodology, because understanding the general work flow of our experiments helps understanding the purpose of the term scoring methods we implemented.Our approach comprises four steps:one. We do not to apply filtering for partof-speech patterns because it cannot be known in advance which POS-patterns are relevant for the collection. For example, for some domains we might only be interested in noun phrases as terms, while for another domain verb phrases are important too..\n",
      "\n",
      "\n",
      "Section: Scoring all candidate terms\n",
      "====================================\n",
      "Summary: We implemented the methods described in Sects three.two and three.three.\n",
      "\n",
      "\n",
      "Section: 3.\n",
      "===========\n",
      "Summary: Ranking the terms by their score.Depending on the context in which the terms are used, a top-k of the ranked list is returned.\n",
      "\n",
      "\n",
      "Section: Term scoring\n",
      "=====================\n",
      "Summary: two thousand and eight).The goal of keyword extraction, as we defined it in Sect. two); the output is a score for each candidate term, higher scores indicating more relevant terms\n",
      "\n",
      "\n",
      "Section: Term scoring methods\n",
      "=============================\n",
      "Summary: For example, the most frequent non-stopwords in this manuscript are 'terms', 'collection', 'background', 'query' and 'method'. Of these, the first four would be relevant descriptors of this paper, but the last one ('method') is very generic. three.five, we summarize the scoring functions and formulate hypotheses on their strengths..\n",
      "\n",
      "\n",
      "Section: Methods for scoring the informativeness of terms\n",
      "=========================================================\n",
      "Summary: We evaluate four methods that address the informativeness of terms: Parsimonious language models (PLM) by Hiemstra et al (two thousand and four), Kullback-Leibler divergence for informativeness (KLI) by Tomokiyo and Hurst (two thousand and three), Frequency Profiling by Rayson and Garside (two thousand) and the Co-occurrence based method (CB) by Matsuo and Ishizuka (two thousand and four).Informativeness methods combine frequency with specificity as measure for the relevance of a term.\n",
      "\n",
      "\n",
      "Section: Parsimonious language models (PLM)\n",
      "===========================================\n",
      "Summary: PLM (Hiemstra et al\n",
      "\n",
      "\n",
      "Section: Kullback-Leibler divergence for informativeness (KLI)\n",
      "==============================================================\n",
      "Summary: Since D is not by definition included in C, there may be terms in D that do not occur in C. For these terms, we estimate P(t|C) as one / |C|, in which |C| is the number of words in the background collection\n",
      "\n",
      "\n",
      "Section: Frequency profiling (FP)\n",
      "=================================\n",
      "Summary: The expected frequencies of a term in D and C are calculated as follows:Eðt; DÞ ¼ jDj countðt; DÞ þ countðt; CÞ jDj þ jCj ðfourÞEðt; CÞ ¼ jCj countðt; DÞ þ countðt; CÞ jDj þ jCj ðfiveÞThen, the log-likelihood ratio test (-twoLL, as in the original paper) is defined as: The term with the largest LL value is the word with the most significant relative frequency difference between the two corpora. The scoring function for FP is similar to the scoring function for KLI. In other words, FP does not only generate terms that are informative for the foreground collection, but also terms that are informative for the background collection.LL ¼ two Ã.\n",
      "\n",
      "\n",
      "Section: Co-occurrence based v 2 (CB)\n",
      "=====================================\n",
      "Summary: The rationale of this method is that no background corpus is needed because the set of most frequent terms from the foreground collection serves as background model. However, there are many cases where multiwords are highly informative terms. (two thousand) and Kullback-Leibler divergence for Phraseness as proposed by Tomokiyo and Hurst (two thousand and three)..\n",
      "\n",
      "\n",
      "Section: C-Value\n",
      "================\n",
      "Summary: This method (Frantzi et al. two thousand) was designed for the recognition of multi-word terms. The score for t is discounted with the average frequencies of all t zero two T t ..\n",
      "\n",
      "\n",
      "Section: Kullback-Leibler divergence for phraseness (KLP)\n",
      "=========================================================\n",
      "Summary: As explained in Sect. KLP is defined as:KLPðtÞ ¼ PðtjDÞ log PðtjDÞ Q n i¼one Pðyou i jDÞðelevenÞin which P(t|D) is the probability of t in D and Pðyou i jDÞ is the probability of the ith unigram inside the n-gram t. The intuition is that (a) longer terms get higher weights than shorter terms and (b) relatively frequent multi-word terms that contain at least one low-frequent unigram (e.g. 'ad hoc', 'latent semantic analysis') are the strongest phrases..\n",
      "\n",
      "\n",
      "Section: Combining informativeness and phraseness\n",
      "=================================================\n",
      "Summary: The only method that has both an informativeness and a phraseness component is KLIP (Tomokiyo and Hurst two thousand and three). twelve: The parameter c two ½zero; one is the weight of the informativeness score KLI(t) relative to the phraseness score KLP(t):scoreðtÞ ¼ c Á KLIðtÞ þ ðone À cÞ Á KLPðtÞ ðthirteenÞWe investigate the effect of c in Sect\n",
      "\n",
      "\n",
      "Section: Hypotheses: strengths of the term scoring methods\n",
      "==========================================================\n",
      "Summary: The same holds for FP, which is similar to KLIP, and was developed for corpus profiling\n",
      "\n",
      "\n",
      "Section: Evaluation collections\n",
      "===============================\n",
      "Summary: The subsections below describe the four ollections that we use for evaluation. In each subsection, we define the use cases in terms of task, collection and evaluation method. Table three at the end of this section shows a summary of the collections..\n",
      "\n",
      "\n",
      "Section: Author profiling using a personal scientific document collection\n",
      "=========================================================================\n",
      "Summary: Several applications can be envisioned that help knowledge workers to manage (incoming) information: just-in-time recommendation of documents, the automatic filtering of e-mail messages and the personalization of search results. Such a term profile could also be published as an author profile in a digital library or on a personal profile page such as LinkedIn. In the remainder of the article they are referred to by the phrases in boldface.\n",
      "\n",
      "\n",
      "Section: Task\n",
      "=============\n",
      "Summary: The term scoring algorithm generates terms from a collection of documents and presents them to the user in a ranked list.\n",
      "\n",
      "\n",
      "Section: Collection and preprocessing\n",
      "=====================================\n",
      "Summary: two thousand and thirteen). The collections consisted of twenty-two English-language documents on average per user (mainly scientific articles) with an average total of sixty-three,nine hundred and thirty-eight words per collecton (standard deviation: thirteen,five hundred and eighty-three). The document collections were preprocessed by converting each document (from PDF or docx) to plain text and split them in sentences..\n",
      "\n",
      "\n",
      "Section: Evaluation method\n",
      "==========================\n",
      "Summary: A pool of one hundred and fifty terms that were scored using three term scoring methods (Hiemstra et al. We asked them to indicate which of the terms are relevant for their work (a binary judgment). There was a large deviation in how many terms were judged as relevant by the users (between twenty-four and fifty-one %), and on average, thirty-six % of the generated terms was perceived as relevant (Verberne et al..\n",
      "\n",
      "\n",
      "Section: Query term suggestion for news monitoring (QUINN)\n",
      "==========================================================\n",
      "Summary: This can happen when new terminology becomes relevant for the topic, there is a new stakeholder or new geographical names are relevant to the topic. This approach is related to pseudo-relevance feedback (Cao et al. two thousand and fifteenb)..\n",
      "\n",
      "\n",
      "Section: Task_1\n",
      "===============\n",
      "Summary: Given a Boolean query, the term scoring algorithm generates terms from the subcollection of documents matching the query and published in the last thirty days, and presents them to the user in a ranked list.\n",
      "\n",
      "\n",
      "Section: Collection and preprocessing_1\n",
      "=======================================\n",
      "Summary: two thousand and fifteenb). Together, the users issued eighty-three searches on LexisNexis' Dutch newspaper collection. We used the LexisNexis Publisher API to retrieve documents (news articles) published in the last thirty days..\n",
      "\n",
      "\n",
      "Section: Evaluation method_1\n",
      "============================\n",
      "Summary: For each method, the top five terms are added to the pool. They are ranked by the number of votes they get (the number of methods for which they appear in the top-five extracted terms). In the experimental interface, the user issues a query in LexisNexis Publisher..\n",
      "\n",
      "\n",
      "Section: Personalized query suggestion\n",
      "======================================\n",
      "Summary: However, for search tasks addressing highly specialized topics, where there are no relevant queries from other users available, the alternative is to fall back to the user's own data (Shen et al. Thus, term extraction in this task is directed at generating potential query terms from relevant documents. For each topic, a subcollection of relevant documents is created using the relevance judgments provided with the data, as source for term extraction..\n",
      "\n",
      "\n",
      "Section: Task_2\n",
      "===============\n",
      "Summary: The term scoring algorithm generates candidate query terms from the subcollection of relevant documents and presents these terms (extensions or adaptations of the previous query) to the user in a ranked list.\n",
      "\n",
      "\n",
      "Section: Collection and preprocessing_2\n",
      "=======================================\n",
      "Summary: The iSearch collection of academic information seeking behavior (Lykke et al. two thousand and ten) consists of sixty-five English-language natural search tasks (topics) from twenty-three researchers and students from university physics departments. The topic owners filled in a form with five fields, among which an explicit description of their information need, and a list of search terms that they would use to express this information need..\n",
      "\n",
      "\n",
      "Section: Evaluation method_2\n",
      "============================\n",
      "Summary: 'Induced-charged electro-osmosis', 'Coupled photonic crystal cavity lasers'). Therefore we expect a relatively low Average Precision for this task. Since we are interested in the relative performance of the methods we evaluate, this is not necessarily problematic: the higher the ranks of the reference terms in the returned term list, the better the term scoring method..\n",
      "\n",
      "\n",
      "Section: Medical query expansion for patient queries\n",
      "====================================================\n",
      "Summary: They compare multiple operators in the Indri query language to combine terms: one() (treating the string between brackets as a literal phrase) combine() (treating the string between brackets as a bag of words) and uwN() (all words between brackets must appear within window of length N in any order). ten They find that uwN is the most powerful operator\n",
      "\n",
      "\n",
      "Section: Task_3\n",
      "===============\n",
      "Summary: The term scoring algorithm generates terms from the discharge summary to be added to the query.\n",
      "\n",
      "\n",
      "Section: Collection and preprocessing_3\n",
      "=======================================\n",
      "Summary: fifty test topics (layman's information needs in English) with a discharge summary for each topic. A topic in the CLEF-eHealth task consists of five descriptive fields: title, description, profile and narrative. We use the title field, or the title together with the description as query..\n",
      "\n",
      "\n",
      "Section: Evaluation method_3\n",
      "============================\n",
      "Summary: We do not have a list of relevant terms from the discharge summary. With the resulting expanded query, one hundred documents are retrieved from the CLEF collection and ranked using the Indri LM with Dirichlet smoothing. We evaluate the retrieval effectiveness in terms of nDCG, one of the most used evaluation measures for ranked retrieval (Järvelin and Kekäläinen two thousand and two)..\n",
      "\n",
      "\n",
      "Section: Experiments with term scoring methods\n",
      "==============================================\n",
      "Summary: one with a series of experiments:one. Each subsection is concluded with a discussion of the experimental results in the light of the hypotheses in Sect\n",
      "\n",
      "\n",
      "Section: What is the influence of the collection size?\n",
      "======================================================\n",
      "Summary: Table five shows the sizes of the four document collections. In Personalized Query Suggestion, the number of documents is reasonable, but the documents are also relatively short, since they consist of metadata or the first two hundred words of a pdf. The collections in Medical Query Expansion are the smallest, with only one document of six hundred and nine words on average per topic..\n",
      "\n",
      "\n",
      "Section: The influence of collection size on the effectiveness of term scoring\n",
      "==============================================================================\n",
      "Summary: The reason that we increase the size of the corpus by paragraph and not by document, is that documents are relatively long and covering one topic each, as a result of which the presence or absence of a complete document will strongly influence the presence or absence of topics in the list of extracted terms, especially in the smaller collections. The randomized sampling of paragraphs ensures a smoother curve. The owners of this corpus provide a word frequency list and n-gram frequency lists that are free to download..\n",
      "\n",
      "\n",
      "Section: Comparing methods for small data collections\n",
      "=====================================================\n",
      "Summary: In order to evaluate the term scoring methods, we extract terms from the discharge summary belonging to the topic and add an increasing number of top-ranked terms (zero,two,five,ten,twenty) to the query. We experiment on the training set provided by CLEF (five topics) with the following settings for query expansion: Mean average precision (over five users).\n",
      "\n",
      "\n",
      "Section: Collection size per user (# of words)\n",
      "==============================================\n",
      "Summary: The bottom row of Table seven shows an example of an expanded Indri query. More specific terms, such as medicine names (e.g..\n",
      "\n",
      "\n",
      "Section: Discussion: What is the influence of the collection size?\n",
      "==================================================================\n",
      "Summary: The poorest performing method is CB. Type-token ratio (TTR) is a measure of lexical variety: it gives the ratio between the number of unique words (types) and the total number of words (tokens) in a corpus..\n",
      "\n",
      "\n",
      "Section: What is the influence of the background collection?\n",
      "============================================================\n",
      "Summary: The choice of the background collection depends on the language and domain of the foreground collection, and on the purpose of the term extraction In this section, we.\n",
      "\n",
      "\n",
      "Section: type-token ratio corpus size\n",
      "=====================================\n",
      "Summary: Each point in the author profiling graph is an average over five runs like in Fig. Each dot in the Medical Query Expansion graph represents one discharge summary (the foreground collection for one topic)evaluate the effect of the background corpus in three informativeness methods (PLM, KLIP (KLI) and FP), for two collections: Personalized Query Suggestion, where we compare a generic and a domain-specific background corpus, and QUINN, where we compare the use of an external background corpus (a Dutch news corpus) and the use of a topic-specific collection: an older subcollection of documents for the same query..\n",
      "\n",
      "\n",
      "Section: Comparing methods with different background corpora in the personalized query suggestion collection\n",
      "============================================================================================================\n",
      "Summary: The results are in Fig. The plot shows that (a) Mean Average Precision is low for this collection..\n",
      "\n",
      "\n",
      "Section: MAP lambda\n",
      "===================\n",
      "Summary: \n",
      "\n",
      "\n",
      "Section: COCA iSearch\n",
      "=====================\n",
      "Summary: For FP, this difference is significant at the zero.five-level. Many terms overlap, although their ranking is different.In Sect..\n",
      "\n",
      "\n",
      "Section: Comparing methods with different background corpora in the QUINN collection\n",
      "====================================================================================\n",
      "Summary: Since the QUINN collection is Dutch, we use the newspaper section from the SoNaR-corpus (Oostdijk et al. two thousand and eight), fifty Million words in total, for this purpose. A McNemar's test for paired binary samples fifteen shows that the difference between the two corpora is significant on the zero.one level for PLM (p ¼ zero:thirty-six) and on the zero.five level for FP (p ¼ zero:thirty-seven) and KLIP (p ¼ zero:thirty-four)..\n",
      "\n",
      "\n",
      "Section: % of searches with term from top-5 selected by the user\n",
      "================================================================\n",
      "Summary: \n",
      "\n",
      "\n",
      "Section: Generic newspaper background corpus\n",
      "============================================\n",
      "Summary: five The quality of the suggested query terms in QUINN, using three different methods and two different background corpora, in terms of the percentage of searches with a term from top-five selected by the user news from the last thirty days and the news on the same topic from sixty-thirty days ago leads to terms that are very specific for the most recent developments on the topic. Hence, the second example topic contains a few names of places (Westrozebeke, Moorslede) that were in the news during the last thirty days. This leads us to the conclusion that a domain-specific background corpus is good, but this domain should not be too narrow (such as a corpus covering one news topic)..\n",
      "\n",
      "\n",
      "Section: Discussion: what is the influence of the background collection?\n",
      "========================================================================\n",
      "Summary: We already noted in Sect. This confirms the second part of our hypothesis..\n",
      "\n",
      "\n",
      "Section: What is the influence of multi-word phrases?\n",
      "=====================================================\n",
      "Summary: We investigate the balance between informativeness and phraseness for the two collections for which we have ground truth terms available: Author Profiling and Personalized Query Suggestion We run KLIP on both collections In Personalized Query.\n",
      "\n",
      "\n",
      "Section: Mean Average Precision gamma\n",
      "=====================================\n",
      "Summary: We evaluate values for c in Eq. thirteen ranging from zero.zero (Phraseness only) to one.zero (Informativeness only) with steps of zero.one The results are in Fig\n",
      "\n",
      "\n",
      "Section: Discussion: What is the influence of multi-word phrases?\n",
      "=================================================================\n",
      "Summary: 'ad hoc', 'new york') are the strongest phrases. boottocht 'boat trip'..\n",
      "\n",
      "\n",
      "Section: Conclusion\n",
      "===================\n",
      "Summary: two thousand and four). However, we did not find strong evidence for one method being better than the others in all scenarios. More work is needed on finding the best strategy for Personalized Query Suggestion in a complex topic domain (Verberne et al..\n",
      "\n",
      "\n",
      "Section: \n",
      "=========\n",
      "Summary: Acknowledgments This publication was supported by the Dutch national program COMMIT (project Pseven SWELL)Open Access This article is distributed under the terms of the Creative Commons Attribution four.zero International License ( which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert your document path here (either URL or direct path)\n",
    "path = \"C:/Users/noudy/Downloads/s10791-016-9286-2.pdf\"\n",
    "\n",
    "summarizer = ScientificLiteratureSummarizer(path)\n",
    "result = summarizer.summarize()\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(f\"\\nSection: {key}\\n{'=' * len('Section: ' + key)}\\nSummary: {value}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:17:40.528807400Z",
     "start_time": "2023-05-23T14:17:00.625376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
