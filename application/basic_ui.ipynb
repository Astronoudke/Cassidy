{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-23T12:22:00.571802300Z",
     "start_time": "2023-05-23T12:21:34.871538800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\noudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\noudy\\\\PycharmProjects\\\\Cassidy\\\\Cassidy\\\\application')\n",
    "\n",
    "from A_DataCollectors.ScientificLiteratureCollector.scientific_literature_collector import ScientificLiteratureCollector\n",
    "from C_DataProcessors.text_preprocessor import TextPreprocessor\n",
    "from D_Analyzers.Summarization.extractive_summarizer import ExtractiveSummarizer\n",
    "\n",
    "\n",
    "class ScientificLiteratureSummarizer:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.format = 'pdf'\n",
    "        self.source_type = 'url' if path.startswith('http') else 'local'\n",
    "        self.method = 'scipy'\n",
    "\n",
    "    def summarize(self):\n",
    "        # Collect data\n",
    "        collector = ScientificLiteratureCollector(self.path)\n",
    "        text = collector.collect(self.format, self.source_type, self.method)\n",
    "\n",
    "        # Preprocess data\n",
    "        summarization_steps = ['clean_data', 'split_sentences']\n",
    "        preprocessor = TextPreprocessor(summarization_steps)\n",
    "        preprocessed_text = preprocessor.preprocess_grobid(text)\n",
    "\n",
    "        # Summarize data\n",
    "        new_dict = {}\n",
    "        for header, sentences in preprocessed_text.items():\n",
    "            es = ExtractiveSummarizer(sentences)\n",
    "            summary = es.summarize('textrank', top_n=3, order_by_rank=False)\n",
    "\n",
    "            new_dict[header] = summary\n",
    "\n",
    "        return new_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noudy\\PycharmProjects\\Cassidy\\venv\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section: title\n",
      "==============\n",
      "Summary: SciBERTSUM: Extractive Summarization for Scientific Documents\n",
      "\n",
      "\n",
      "Section: Introduction\n",
      "=====================\n",
      "Summary: Pre-trained language models are easy to incorporate and do not require much-labeled data to deal with, which makes it appropriate for many problems such as prediction, transfer learning, and feature extraction.. Bidirectional Encoder Representations from Transformers (BERT) [six] have combined both word and sentence representations into a single very large Transformer [twenty-three].. The slides contain the technical details from the paper and usually follow the structure of the papers.two Related Work\n",
      "\n",
      "\n",
      "Section: Summarization\n",
      "======================\n",
      "Summary: However, abstracts are extremely compressed versions of a papers and usually do not have enough space to include all of the contributions [seven].. -Speaker Transcript: Many conference proceedings/ workshops require the authors to verbally present their work.. We used the PSfivek dataset [twenty,twenty-one] to build our summarizer.\n",
      "\n",
      "\n",
      "Section: Transformer Based Summarization\n",
      "========================================\n",
      "Summary: Pre-trained language models such as BART [eleven] produce state-of-the-art results on the summarization tasks.. Sotudeh et al.. [twenty-two] added section information to the objective function of BERTSUM so it could optimize both the sentence prediction and section prediction tasks in a multi-task setting.\n",
      "\n",
      "\n",
      "Section: Method -SciBERTSUM\n",
      "===========================\n",
      "Summary: Most of the previous language models such as BERT are employed as encoders for short pieces of text usually covering at most two sentences.. The summarization task besides other NLP tasks (e.g.. We propose a document encoder based on BERT.\n",
      "\n",
      "\n",
      "Section: Language Model Architecture\n",
      "====================================\n",
      "Summary: To explain the architecture of our language model, we first explain how we generate the sentence embeddings by adding section information to sentences and then we explain how our sparse attention mechanism helps us process the full document efficiently.\n",
      "\n",
      "\n",
      "Section: Embedding Layer\n",
      "========================\n",
      "Summary: al [twenty-three].. Long documents especially long scholarly articles contain multiple sections.. We can load the maximum three thousand and seventy-two tokens to the memory based on experiments on an Nvidia GPU with eleven,nineteen MiB memory capacity.\n",
      "\n",
      "\n",
      "Section: Attention Mechanism\n",
      "============================\n",
      "Summary: two.. three.. For example, the first document attends locally at positions [one,two,three,four,five,six ] and attends globally at position four.\n",
      "\n",
      "\n",
      "Section: Fig. 5: Attention Mask Matrix\n",
      "======================================\n",
      "Summary: two.. four.. (nine)\n",
      "\n",
      "\n",
      "Section: Transformer Layer\n",
      "==========================\n",
      "Summary: Our sparse attention mechanism is applied in each layer of transformation [twenty-three] and the inputs to the first layer of transformer are the sentence embeddings that include the section informationh l = h l−one + normalize(SparseAttention(h l−one ))(ten)h l = P ositionwiseF eedF orward( h l ) (eleven)where h zero = E sents .. We apply a sparse attention mechanism here instead of full attention.\n",
      "\n",
      "\n",
      "Section: Sentence Extractor\n",
      "===========================\n",
      "Summary: To generate the final sentence score, we combined the sentence embedding from the language module with a list of features necessary for the score prediction.. Section five.one elaborates on the list of features applied to generate the sentence scores.. These features depend on the document embedding calculated as in five.two.\n",
      "\n",
      "\n",
      "Section: Sentence Features\n",
      "==========================\n",
      "Summary: The features used to predict the final scores are:one.. four.. five.\n",
      "\n",
      "\n",
      "Section: Document Embedding\n",
      "===========================\n",
      "Summary: The document encoder is a simply the weighted average of the sentence vectors:W eight = Sof tmax(E sents × W sents )(nineteen)where E sents ∈ R n×d and W sents ∈ R d×one .. The wights are initialized randomly and will be learned during the training process.. Therefore, W eight ∈ R n×one are the weights of the sentences.Therefore, the embedding of document D is:E D = one n n i=one W eight[i] * E sents[i](twenty)where the terms are defined above.\n",
      "\n",
      "\n",
      "Section: Score Predictor\n",
      "========================\n",
      "Summary: The score prediction module concatenates all of the features and feeds them to a linear layer to generate the final scores.. Our cross-entropy loss evaluates the difference between the prediction and the ground truth scores.. We also evaluated the loss factored by the rewards to see if the model makes better predictions using reinforcement learning (in section six)p(y i ) = Linear(E sent + E length + E position + E section + E correlation + E saliency )(twenty-one)where p(y i ) is the probability of adding sentence i to the summary and the linear layer format is W x T + b and x ∈ R one×d and w ∈ R one×d .\n",
      "\n",
      "\n",
      "Section: Reinforcement Learning\n",
      "===============================\n",
      "Summary: The Objective function is to minimize this negative log likelihoodloss = − n i=one log[p(y i )].. (twenty-two)The objective in Eq.. twenty-two maximizes the correct prediction of zero/one labels where p(y i ) is the probability of label y i for sentence i.\n",
      "\n",
      "\n",
      "Section: Experimental Results\n",
      "=============================\n",
      "Summary: \n",
      "\n",
      "\n",
      "Section: Hardware\n",
      "=================\n",
      "Summary: Three NVIDIA GPUs (GeForce RTX two thousand and eighty Ti) with eleven thousand and nineteenMiB on memory were used.. Since we could not have a large batch size, we accumulate the gradients for ten steps and then update the parameters.. We used the NOAM scheduler to adjust the learning rate while training and apply gradient-clipping to prevent exploding gradients.\n",
      "\n",
      "\n",
      "Section: Experiments\n",
      "====================\n",
      "Summary: We also tried allowing some shared trigram in the sentence.. scientific papers).. It would also be interesting to see whether the SciBERT language model, which is pre-trained on scientific text, will give improved performance.\n",
      "\n",
      "\n",
      "Section: Acknowledgement\n",
      "========================\n",
      "Summary: Partial support from the National Science Foundation is gratefully acknowledged.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert your document path here (either URL or direct path)\n",
    "path = \"https://arxiv.org/pdf/2201.08495.pdf\"\n",
    "\n",
    "summarizer = ScientificLiteratureSummarizer(path)\n",
    "result = summarizer.summarize()\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(f\"\\nSection: {key}\\n{'=' * len('Section: ' + key)}\\nSummary: {value}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T12:34:27.301718100Z",
     "start_time": "2023-05-23T12:34:20.601787900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Section                         | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+=================================+======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================+\n",
      "| title                           | SciBERTSUM: Extractive Summarization for Scientific Documents                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Introduction                    | Pre-trained language models are easy to incorporate and do not require much-labeled data to deal with, which makes it appropriate for many problems such as prediction, transfer learning, and feature extraction.. Bidirectional Encoder Representations from Transformers (BERT) [six] have combined both word and sentence representations into a single very large Transformer [twenty-three].. The slides contain the technical details from the paper and usually follow the structure of the papers.two Related Work                                                                                                          |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Summarization                   | However, abstracts are extremely compressed versions of a papers and usually do not have enough space to include all of the contributions [seven].. -Speaker Transcript: Many conference proceedings/ workshops require the authors to verbally present their work.. We used the PSfivek dataset [twenty,twenty-one] to build our summarizer.                                                                                                                                                                                                                                                                                        |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Transformer Based Summarization | Pre-trained language models such as BART [eleven] produce state-of-the-art results on the summarization tasks.. Sotudeh et al.. [twenty-two] added section information to the objective function of BERTSUM so it could optimize both the sentence prediction and section prediction tasks in a multi-task setting.                                                                                                                                                                                                                                                                                                                  |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Method -SciBERTSUM              | Most of the previous language models such as BERT are employed as encoders for short pieces of text usually covering at most two sentences.. The summarization task besides other NLP tasks (e.g.. We propose a document encoder based on BERT.                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Language Model Architecture     | To explain the architecture of our language model, we first explain how we generate the sentence embeddings by adding section information to sentences and then we explain how our sparse attention mechanism helps us process the full document efficiently.                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Embedding Layer                 | al [twenty-three].. Long documents especially long scholarly articles contain multiple sections.. We can load the maximum three thousand and seventy-two tokens to the memory based on experiments on an Nvidia GPU with eleven,nineteen MiB memory capacity.                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Attention Mechanism             | two.. three.. For example, the first document attends locally at positions [one,two,three,four,five,six ] and attends globally at position four.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Fig. 5: Attention Mask Matrix   | two.. four.. (nine)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Transformer Layer               | Our sparse attention mechanism is applied in each layer of transformation [twenty-three] and the inputs to the first layer of transformer are the sentence embeddings that include the section informationh l = h l−one + normalize(SparseAttention(h l−one ))(ten)h l = P ositionwiseF eedF orward( h l ) (eleven)where h zero = E sents .. We apply a sparse attention mechanism here instead of full attention.                                                                                                                                                                                                                   |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Sentence Extractor              | To generate the final sentence score, we combined the sentence embedding from the language module with a list of features necessary for the score prediction.. Section five.one elaborates on the list of features applied to generate the sentence scores.. These features depend on the document embedding calculated as in five.two.                                                                                                                                                                                                                                                                                              |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Sentence Features               | The features used to predict the final scores are:one.. four.. five.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Document Embedding              | The document encoder is a simply the weighted average of the sentence vectors:W eight = Sof tmax(E sents × W sents )(nineteen)where E sents ∈ R n×d and W sents ∈ R d×one .. The wights are initialized randomly and will be learned during the training process.. Therefore, W eight ∈ R n×one are the weights of the sentences.Therefore, the embedding of document D is:E D = one n n i=one W eight[i] * E sents[i](twenty)where the terms are defined above.                                                                                                                                                                     |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Score Predictor                 | The score prediction module concatenates all of the features and feeds them to a linear layer to generate the final scores.. Our cross-entropy loss evaluates the difference between the prediction and the ground truth scores.. We also evaluated the loss factored by the rewards to see if the model makes better predictions using reinforcement learning (in section six)p(y i ) = Linear(E sent + E length + E position + E section + E correlation + E saliency )(twenty-one)where p(y i ) is the probability of adding sentence i to the summary and the linear layer format is W x T + b and x ∈ R one×d and w ∈ R one×d . |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Reinforcement Learning          | The Objective function is to minimize this negative log likelihoodloss = − n i=one log[p(y i )].. (twenty-two)The objective in Eq.. twenty-two maximizes the correct prediction of zero/one labels where p(y i ) is the probability of label y i for sentence i.                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Experimental Results            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Hardware                        | Three NVIDIA GPUs (GeForce RTX two thousand and eighty Ti) with eleven thousand and nineteenMiB on memory were used.. Since we could not have a large batch size, we accumulate the gradients for ten steps and then update the parameters.. We used the NOAM scheduler to adjust the learning rate while training and apply gradient-clipping to prevent exploding gradients.                                                                                                                                                                                                                                                       |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Experiments                     | We also tried allowing some shared trigram in the sentence.. scientific papers).. It would also be interesting to see whether the SciBERT language model, which is pre-trained on scientific text, will give improved performance.                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Acknowledgement                 | Partial support from the National Science Foundation is gratefully acknowledged.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Convert the result into a list of lists for tabulate\n",
    "table = [[key, value] for key, value in result.items()]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table, headers=[\"Section\", \"Summary\"], tablefmt=\"grid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T12:37:06.031268Z",
     "start_time": "2023-05-23T12:37:05.982847400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
