The goal of automatic term extraction is to extract and rank the most relevant terms from a document or a document collection. The central methodology needed for term extraction is term scoring: each candidate term from the document (collection) is assigned a score that allows for selecting the best—most relevant—terms. What factors determine the success of a term scoring method for keyword extraction? In this paper, we evaluate and compare six unsupervised term scoring methods from the literature on four different test collections, each with their own specific use case. We investigated the influence of three factors in the success of a term scoring method in term extraction: collection size, background collection and the importance of multi-word terms. for collections larger than 10,000 words, the best performing method for the task of author profiling is Kullback–Leibler divergence for Informativeness and Phraseness. for modeling smaller collections up to 5,000 words, the best performing method for the task of author profiling is Parsimonious Language Models. With respect to the importance of multi-word terms, our results and analyses indicate that KLIP is the most flexible method for extracting both single-word terms and multi-word terms. With respect to use of a background collection, our methodological analyses indicate that PLM would be a good choice in situations where the foreground collection or document is embedded in a larger collection, and KLIP would be a good choice for extracting terms from a larger collection that does not have an overarching background collection. Our final recommendation is that the choice of method and evaluation for term extraction should depend on the specific use case.